#+TITLE: Computational Linguistics Notes


* Basic Tokenizing
The example below creates the so-called *one-hot vectors*,
which store in a matrix the words in a sentence, in alphabetical
order. We will use the sentence:

#+BEGIN_QUOTE
Thomas Jefferson began building Monticello at the age of 26.
#+END_QUOTE

After sorting it alphabetically (numbers before letters, capitals
before lower case), we get:
#+BEGIN_QUOTE
26. Jefferson Monticello Thomas age at began building of the
#+END_QUOTE

Then we create a 10x10 matrix to store the words. Each column of
the matrix represents a word, so the first column is for =26=,
the second is for =Jefferson= etc. The matrix is initially filled
with zeros and for each column (word), we find =1= at line =j=
if the word appears at position =j= in the /initial/ sentence
(the unsorted version, defined using natural language).

The code follows.
#+BEGIN_SRC python :tangle code/simple_tok.py :exports code :results output
  # FILE: simple_tok.py
  #############################################################
  # This file should be read and used in conjuction with the
  # notes contained in the notes.org file in the current repo.
  # Explanations and comments are found there. This is merely
  # an exported standalone version to be interpreted outside
  # the notes.
  #############################################################

  import numpy as np

  sentence = """Thomas Jefferson began building Monticello
				at the age of 26."""

  token_sequence = str.split(sentence)
  vocab = sorted(set(token_sequence))
  ', '.join(vocab)
  num_tokens = len(token_sequence)
  vocab_size = len(vocab)
  onehot_vectors = np.zeros((num_tokens, vocab_size), int)
  for i, word in enumerate(token_sequence):
	  onehot_vectors[i, vocab.index(word)] = 1
  ' '.join(vocab)
  print(onehot_vectors)
#+END_SRC

Which produces:

#+begin_example
[[0 0 0 1 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [1 0 0 0 0 0 0 0 0 0]]
#+end_example


A better visualisation of the information can be done either with
=Pandas=, which lists the table headers or with =DataFrame=, which
omits the zeros.

This way, we get (notice the explicit direction to =$pwd= and the =import=
of previous file):
#+BEGIN_SRC python :tangle code/simple_tok_pretty.py :exports code :results output
  import sys
  sys.path.append('~/repos/0-mine/compling/code/')

  import simple_tok as st
  import pandas as pd

  print("Pandas print:")
  print(pd.DataFrame(st.onehot_vectors, columns=st.vocab))

  df = pd.DataFrame(st.onehot_vectors, columns=st.vocab)
  df[df == 0] = ''                # ignore zeros
  print("Prettier, no zeros:")
  print(df)
#+END_SRC

Which produces:

#+begin_example
Pandas print:
   26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the
0    0          0           0       1    0   0      0         0   0    0
1    0          1           0       0    0   0      0         0   0    0
2    0          0           0       0    0   0      1         0   0    0
3    0          0           0       0    0   0      0         1   0    0
4    0          0           1       0    0   0      0         0   0    0
5    0          0           0       0    0   1      0         0   0    0
6    0          0           0       0    0   0      0         0   0    1
7    0          0           0       0    1   0      0         0   0    0
8    0          0           0       0    0   0      0         0   1    0
9    1          0           0       0    0   0      0         0   0    0
Prettier, no zeros:
  26. Jefferson Monticello Thomas age at began building of the
0                               1                             
1             1                                               
2                                            1                
3                                                     1       
4                        1                                    
5                                      1                      
6                                                            1
7                                   1                         
8                                                        1    
9   1        
#+end_example

* Dot Product Calculation
We will use the dot product in many situations for computing
similarity. A basic example, using =Pandas= and =numpy= is
the following:
#+BEGIN_SRC python :tangle code/dot_product.py :exports both :results output
  import pandas as pd
  import numpy as np

  v1 = pd.np.array([1, 2, 3])
  v2 = pd.np.array([2, 3, 4])
  print("The dot product of the two vectors is:")
  print(v1.dot(v2))

  print("Manual computation of the dot product:")
  print(sum([x1 * x2 for x1, x2 in zip(v1, v2)]))
#+END_SRC

which outputs:

#+RESULTS:
: The dot product of the two vectors is:
: 20
: Manual computation of the dot product:
: 20

* Better Tokenizing
Some of the most commonly used methods involve:
- =re= or =regex= (newer) packages for Regular Expression support;
- Stanford CoreNLP: accurate, less flexible, depends on Java;
- NLTK: standard;
- spaCy: newcomer.

Simple examples follow.

RegEx:
#+BEGIN_SRC python :tangle code/re_tok.py :exports both :results output
  import re

  sentence = """Thomas Jefferson began building Monticello at the\
  age of 26."""

  # regex to split at whitespace or punctuation
  # that appears at least once
  tokens = re.split(r'[-\s.,;!?]+', sentence)

  print(tokens)
#+END_SRC

which produces:

#+RESULTS:
: ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'theage', 'of', '26', '']

Precompiled RegEx passed to pattern matching:
#+BEGIN_SRC python :tangle code/pattern_tok.py :exports both :results output
  import re

  sentence = """Thomas Jefferson began building the Monticello at the\
  age of 26."""

  # prepare the regex beforehand
  pattern = re.compile(r"([-\s.,;~?])+")

  # apply the regex
  tokens = pattern.split(sentence)

  # filter out whitespace and punctuation
  good_toks = [x for x in tokens if x and x not in '- \t\n.,;~?']

  print(good_toks)
#+END_SRC

which produces:

#+RESULTS:
: ['Thomas', 'Jefferson', 'began', 'building', 'the', 'Monticello', 'at', 'theage', 'of', '26']

Using NLTK's =RegexpTokenizer=:
#+BEGIN_SRC python :tangle code/nltk_regexp_tok.py :exports both :results output
  from nltk.tokenize import RegexpTokenizer

  sentence = """Thomas Jefferson began building the Monticello at the
  age of 26."""

  tokenizer = RegexpTokenizer(r'\w+|$[0-9.]+|\S+')

  print(tokenizer.tokenize(sentence))
#+END_SRC

which gives:

#+RESULTS:
: ['Thomas', 'Jefferson', 'began', 'building', 'the', 'Monticello', 'at', 'the', 'age', 'of', '26', '.']

Using NLTK's [[http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank][=Penn Treebank Tokenizer=]]:
#+BEGIN_SRC python :tangle code/nltk_treebank_tok.py :exports both :results output
  from nltk.tokenize import TreebankWordTokenizer

  sentence = """Monticello wasn't designed as UNESCO World Heritage\
  Site until 1987."""

  tokenizer = TreebankWordTokenizer()
  print(tokenizer.tokenize(sentence))
#+END_SRC

which gives:

#+RESULTS:
: ['Monticello', 'was', "n't", 'designed', 'as', 'UNESCO', 'World', 'HeritageSite', 'until', '1987', '.']

* N-grams
In some cases, it could be relevant for the meaning of a text
to keep N-grams. For example, if the text contains "ice cream",
it is best interpreted if the two words are /not/ separated. For
such purposes, we use N-grams (2-grams in the case of "ice cream").

Example using NLTK's =ngrams= tokenizer:
#+BEGIN_SRC python :tangle code/nltk_ngrams.py :exports both :results output
  import re
  from nltk.util import ngrams

  sentence = """Thomas Jefferson began building Monticello at the
  age of 26."""

  # get the tokens (words) first with a simple split
  pattern = re.compile(r"([-\s.,;~?])+")
  tokens = pattern.split(sentence)
  # disregard whitespace and punctuation
  tokens = [x for x in tokens if x and x not in '- \t\n.,;~?']

  # N-grams with NLTK
  print("2- and 3-grams as tuples:")
  print(list(ngrams(tokens, 2)))
  print(list(ngrams(tokens, 3)))

  print("2-grams joined with whitespace:")
  two_grams = list(ngrams(tokens, 2))
  print([" ".join(x) for x in two_grams])
#+END_SRC

This gives:
#+RESULTS:
: 2- and 3-grams as tuples:
: [('Thomas', 'Jefferson'), ('Jefferson', 'began'), ('began', 'building'), ('building', 'Monticello'), ('Monticello', 'at'), ('at', 'the'), ('the', 'age'), ('age', 'of'), ('of', '26')]
: [('Thomas', 'Jefferson', 'began'), ('Jefferson', 'began', 'building'), ('began', 'building', 'Monticello'), ('building', 'Monticello', 'at'), ('Monticello', 'at', 'the'), ('at', 'the', 'age'), ('the', 'age', 'of'), ('age', 'of', '26')]
: 2-grams joined with whitespace:
: ['Thomas Jefferson', 'Jefferson began', 'began building', 'building Monticello', 'Monticello at', 'at the', 'the age', 'age of', 'of 26']

* Stopwords
NLTK contains a list of stopwords which one can readily download,
use and query:
#+BEGIN_SRC python :tangle code/nltk_stopwords.py :exports both :results output
  import nltk

  nltk.download('stopwords')
  stop_words = nltk.corpus.stopwords.words('english')
  print("There are " + str(len(stop_words)) + " stopwords in NLTK English DB")

  print("Some of them are:")
  print(stop_words[:10])

  print("The shortest are:")
  print([sw for sw in stop_words if len(sw) == 1])
#+END_SRC

Which gives:
#+RESULTS:
: There are 179 stopwords in NLTK English DB
: Some of them are:
: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]
: The shortest are:
: ['i', 'a', 's', 't', 'd', 'm', 'o', 'y']

* Normalizing the Vocabulary
*Case folding* is simply done with =.lower()=.

*Stemming* can be done manually for disregarding plural =s='s:
#+BEGIN_SRC python :tangle code/stemmer_plurals.py :exports both :results output
  import re

  def stem(phrase):
	  return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip("'")
			   for word in phrase.lower().split()])

  print("Stemming plurals for 'houses' gives: ", end="")
  print(stem('houses'))
  print("Stemming plurals from 'Doctor House's Calls' gives: ", end="")
  print(stem("Doctor House's calls"))
#+END_SRC

This gives:
#+RESULTS:
: Stemming plurals for 'houses' gives: house
: Stemming plurals from 'Doctor House's Calls' gives: doctor house call

NLTK's =PorterStemmer= works like this:
#+BEGIN_SRC python :tangle code/stemmer_nltk_porter.py :exports both :results output
  from nltk.stem.porter import PorterStemmer

  stemmer = PorterStemmer()
  print("Porter Stemmer for 'Dish washer's washed dishes' gives:")
  print(' '.join([stemmer.stem(w).strip("'") for w in
				  "Dish washer's washed dishes".split()]))
#+END_SRC

#+RESULTS:
: Porter Stemmer for 'Dish washer's washed dishes' gives:
: dish washer wash dish

The Porter Stemmer has been ported to pure Python [[https://github.com/jedijulia/porter-stemmer][here]]. The basic steps
are the following:
- *Step 1a*: remove "s" and "es" endings;
- *Step 1b*: remove "ed", "ing" and "at" endings;
- *Step 1c*: remove "y" endings;
- *Step 2*: /nounifying/ endings such as "(a)tional", "ence", "able";
- *Step 3*: adjective endings such as "icate", "ful", "alize";
- *Step 4*: adjectives and nound endings such as "ive", "ible", "ent", "ism";
- *Step 5a*: some "e" endings still left around;
- *Step 5b*: trailing double consonants for which the stem should end in one consonant.

*Lemmatization* is done readily with NLTK:
#+BEGIN_SRC python :tangle code/lemmatizer_nltk.py :exports both :results output
  import nltk
  nltk.download('wordnet')
  from nltk.stem import WordNetLemmatizer

  lemmatizer = WordNetLemmatizer()
  adjectives = ["better", "good", "goods", "best"]
  nouns = ["better", "goods", "goodness"]

  print("> We lemmatize the following adjectives:")
  for adj in adjectives:
	  print(adj, end=" ")
  print("\n> And get respectively:")
  for adj in adjectives:
	  print(lemmatizer.lemmatize(adj, pos="a"), end=" ")

  print("\n> Now the following nouns:")
  for n in nouns:
	  print(n, end=" ")
  print("\n> And get respectively:")
  for n in nouns:
	  print(lemmatizer.lemmatize(n, pos="n"), end=" ")
#+END_SRC

This gives:
#+RESULTS:
: > We lemmatize the following adjectives:
: better good goods best 
: > And get respectively:
: good good goods best 
: > Now the following nouns:
: better goods goodness 
: > And get respectively:
: better good goodness 

* TF-IDF Vectors
** Bag of Words and Cleanup
First, we create a bag of words from a sentence. But we also must
cleanup repeated words. And for this purpose, we will store them
in a dictionary, that has the count aside each word.
#+BEGIN_SRC python :tangle code/bagofwords_count.py :exports both :results output
  from nltk.tokenize import TreebankWordTokenizer
  from collections import Counter

  sentence = """The faster Harry got to the store, the faster
  Harry, the faster, would get home."""

  tokenizer = TreebankWordTokenizer()
  tokens = tokenizer.tokenize(sentence.lower())
  print("Individual tokens (words):")
  for tok in tokens:
	  print(tok, end=" ")

  bag_of_words = Counter(tokens)
  print("\nCleaned up now, with counts as a Counter:")
  print(bag_of_words)

  # collections.Counter objects have a bult-in method
  # which prints the most common occurrences
  print("The 3 most common words are:")
  print(bag_of_words.most_common(3))
#+END_SRC

This gives:
#+RESULTS:
: Individual tokens (words):
: the faster harry got to the store , the faster harry , the faster , would get home . 
: Cleaned up now, with counts as a Counter:
: Counter({'the': 4, 'faster': 3, ',': 3, 'harry': 2, 'got': 1, 'to': 1, 'store': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})
: The 3 most common words are:
: [('the', 4), ('faster', 3), (',', 3)]

** Term Frequency (TF)
One can easily compute the frequency of "harry" in the previous example:
#+BEGIN_SRC python :tangle code/tf_from_Counter.py :exports both :results output
  import sys
  sys.path.append("./code")

  import bagofwords_count as bow

  times_harry_appears = bow.bag_of_words['harry']
  num_unique_words = len(bow.bag_of_words)
  tf = times_harry_appears / num_unique_words
  tf_harry = round(tf, 4)
  print("TF for 'harry' is " + str(tf_harry))
#+END_SRC

This gives:
#+RESULTS:
: Individual tokens (words):
: the faster harry got to the store , the faster harry , the faster , would get home . 
: Cleaned up now, with counts as a Counter:
: Counter({'the': 4, 'faster': 3, ',': 3, 'harry': 2, 'got': 1, 'to': 1, 'store': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})
: The 3 most common words are:
: [('the', 4), ('faster', 3), (',', 3)]
: TF for 'harry' is 0.1818

** Example: Kite Text
The example below downloads the Wikipedia article on kites and
performs basic analysis on it.

#+BEGIN_SRC python :tangle code/tf_example_kite.py :exports both :results output
  from collections import Counter

  from nltk.tokenize import TreebankWordTokenizer
  tokenizer = TreebankWordTokenizer()

  from nlpia.data.loaders import kite_text

  tokens = tokenizer.tokenize(kite_text.lower())
  token_counts = Counter(tokens)

  import nltk
  nltk.download('stopwords', quiet=True)
  stopwords = nltk.corpus.stopwords.words('english')

  tokens = [x for x in tokens if x not in stopwords]
  # word counts from the article, that are NOT stopwords
  kite_counts = Counter(tokens)

  document_vector = []
  doc_length = len(tokens)
  # frequency of each word = apparition count / doc length
  for key, value in kite_counts.most_common():
	  document_vector.append(value / doc_length)

  print("The first 5 frequencies are:")
  print(document_vector[:5])
#+END_SRC

This gives:
#+RESULTS:
: The first 5 frequencies are:
: [0.07207207207207207, 0.06756756756756757, 0.036036036036036036, 0.02252252252252252, 0.018018018018018018]

** Cosine for Similarity
The listing below is computing the similarity cosine for *document vectors*,
as in the example above, so it assumes it is using /dictionaries/, which
are first converted to lists to ignore the words and keep just the frequencies:
#+BEGIN_SRC python :tangle code/cosine.py :exports both :results output
  import math

  def cosine_sim(vec1, vec2):
	  """Let's convert the dictionaries to lists for easier matching."""
	  vec1 = [val for val in vec1.values()]
	  vec2 = [val for val in vec2.values()]

	  dot_prod = 0
	  for i, v in enumerate(vec1):
		  dot_prod += v * vec2[i]

	  mag1 = math.sqrt(sum([x**2 for x in vec1]))
	  mag2 = math.sqrt(sum([x**2 for x in vec2]))

	  return dot_prod / (mag1 * mag2)
#+END_SRC

The numpy alternative for cosine is:
#+BEGIN_SRC python
  a.dot(b) == np.linalg.norm(a) * np.linalg.norm(b) / np.cos(theta)

  # so to get the cosine, we do
  cosine_similarity = a.dot(b) / (np.linalg(norm(a)) * np.linalg.norm(b))
#+END_SRC

** Example: IDF
#+BEGIN_SRC python :tangle code/idf_example.py :exports both :results output
  from collections import OrderedDict, Counter
  from nltk.tokenize import TreebankWordTokenizer
  tokenizer = TreebankWordTokenizer()

  from nlpia.data.loaders import kite_text, kite_history

  kite_intro = kite_text.lower()
  intro_tokens = tokenizer.tokenize(kite_intro)
  kite_history = kite_history.lower()
  history_tokens = tokenizer.tokenize(kite_history)

  intro_total = len(intro_tokens)
  print("The kite text contains " + str(intro_total) + " tokens")
  history_total = len(history_tokens)
  print("The kite history contains " + str(history_total) + " tokens")

  intro_tf = {}
  history_tf = {}
  intro_counts = Counter(intro_tokens)
  intro_tf['kite'] = intro_counts['kite'] / intro_total
  history_counts = Counter(history_tokens)
  history_tf['kite'] = history_counts['kite'] / history_total
  print("Term frequency of 'kite' in intro is " + str(round(intro_tf['kite'], 4)))
  print("Term frequency of 'kite' in history is " + str(round(history_tf['kite'], 4)))

  # maybe the counts are not that relevant compared to 'and'
  intro_tf['and'] = intro_counts['and'] / intro_total
  history_tf['and'] = history_counts['and'] / history_total
  print("Term frequency of 'and' in intro is " + str(round(intro_tf['and'], 4)))
  print("Term frequency of 'and' in history is " + str(round(history_tf['and'], 4)))

  # let's use rarity for IDF
  num_docs_containing_and = 0
  for doc in [intro_tokens, history_tokens]:
	  if 'and' in doc:
		  num_docs_containing_and += 1

  num_docs_containing_kite = 0
  for doc in [intro_tokens, history_tokens]:
	  if 'kite' in doc:
		  num_docs_containing_kite += 1

  num_docs_containing_china = 0
  for doc in [intro_tokens, history_tokens]:
	  if 'china' in doc:
		  num_docs_containing_china += 1

  # TF of "China"
  intro_tf['china'] = intro_counts['china'] / intro_total
  history_tf['china'] = history_counts['china'] / history_total

  # IDF
  num_docs = 2
  intro_idf = {}
  history_idf = {}

  intro_idf['and'] = num_docs / num_docs_containing_and
  intro_idf['kite'] = num_docs / num_docs_containing_kite
  intro_idf['china'] = num_docs / num_docs_containing_china

  history_idf['and'] = num_docs / num_docs_containing_and
  history_idf['kite'] = num_docs / num_docs_containing_kite
  history_idf['china'] = num_docs / num_docs_containing_china

  # TF-IDF
  intro_tfidf = {}
  intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']
  intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']
  intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']

  history_tfidf = {}
  history_tfidf['and'] = history_tf['and'] * history_idf['and']
  history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']
  history_tfidf['china'] = history_tf['china'] * history_idf['china']

  # example prints
  print("TF-IDF for 'kite' in intro text is " + str(round(intro_tfidf['kite'], 4)))
  print("TF-IDF for 'kite' in history text is " + str(round(history_tfidf['kite'], 4)))
#+END_SRC

#+RESULTS:
: The kite text contains 363 tokens
: The kite history contains 297 tokens
: Term frequency of 'kite' in intro is 0.0441
: Term frequency of 'kite' in history is 0.0202
: Term frequency of 'and' in intro is 0.0275
: Term frequency of 'and' in history is 0.0303
: TF-IDF for 'kite' in intro text is 0.0441
: TF-IDF for 'kite' in history text is 0.0202

** Example: Automated TF-IDF with =sklearn=

Tutorials on =scikit-learn= [[https://scikit-learn.org/stable/tutorial/index.html][here]].
#+BEGIN_SRC python :tangle code/tfidf_sklearn.py :exports both :results output
  from sklearn.feature_extraction.text import TfidfVectorizer
  from nlpia.data.loaders import kite_text

  # case fold the text, but store it as a "text object"
  # to be fed later (string not accepted)
  corpus = [kite_text.lower()]

  vectorizer = TfidfVectorizer(min_df=1)
  model = vectorizer.fit_transform(corpus)
  # convert the sparse matrix to a dense numpy-like version
  print(model.todense().round(2))
#+END_SRC

This gives:

#+RESULTS:
#+begin_example
[[0.02 0.02 0.02 0.02 0.09 0.02 0.07 0.02 0.07 0.23 0.02 0.02 0.07 0.02
  0.02 0.12 0.07 0.02 0.02 0.02 0.12 0.02 0.02 0.02 0.02 0.05 0.02 0.05
  0.05 0.07 0.02 0.02 0.02 0.05 0.02 0.02 0.05 0.02 0.05 0.02 0.02 0.07
  0.05 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.07 0.02 0.02 0.05 0.05 0.02
  0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.09 0.02 0.02 0.02 0.02 0.02 0.16
  0.02 0.02 0.16 0.02 0.4  0.19 0.09 0.02 0.02 0.02 0.02 0.09 0.05 0.02
  0.05 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.09 0.02 0.02 0.05 0.02 0.02
  0.02 0.02 0.23 0.05 0.02 0.02 0.14 0.02 0.02 0.02 0.02 0.02 0.05 0.05
  0.02 0.05 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02
  0.07 0.02 0.02 0.02 0.02 0.02 0.02 0.05 0.02 0.02 0.02 0.02 0.05 0.02
  0.05 0.05 0.05 0.05 0.05 0.6  0.02 0.12 0.02 0.02 0.02 0.02 0.02 0.02
  0.05 0.02 0.02 0.02 0.02 0.02 0.02 0.05 0.05 0.07 0.12 0.05 0.05 0.02]]
#+end_example

* Semantics
