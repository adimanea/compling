#+TITLE: Computational Linguistics Notes


* Basic Tokenizing
The example below creates the so-called *one-hot vectors*,
which store in a matrix the words in a sentence, in alphabetical
order. We will use the sentence:

#+BEGIN_QUOTE
Thomas Jefferson began building Monticello at the age of 26.
#+END_QUOTE

After sorting it alphabetically (numbers before letters, capitals
before lower case), we get:
#+BEGIN_QUOTE
26. Jefferson Monticello Thomas age at began building of the
#+END_QUOTE

Then we create a 10x10 matrix to store the words. Each column of
the matrix represents a word, so the first column is for =26=,
the second is for =Jefferson= etc. The matrix is initially filled
with zeros and for each column (word), we find =1= at line =j=
if the word appears at position =j= in the /initial/ sentence
(the unsorted version, defined using natural language).

The code follows.
#+BEGIN_SRC python :tangle code/simple_tok.py :exports code :results output
  # FILE: simple_tok.py
  #############################################################
  # This file should be read and used in conjuction with the
  # notes contained in the notes.org file in the current repo.
  # Explanations and comments are found there. This is merely
  # an exported standalone version to be interpreted outside
  # the notes.
  #############################################################

  import numpy as np

  sentence = """Thomas Jefferson began building Monticello
				at the age of 26."""

  token_sequence = str.split(sentence)
  vocab = sorted(set(token_sequence))
  ', '.join(vocab)
  num_tokens = len(token_sequence)
  vocab_size = len(vocab)
  onehot_vectors = np.zeros((num_tokens, vocab_size), int)
  for i, word in enumerate(token_sequence):
	  onehot_vectors[i, vocab.index(word)] = 1
  ' '.join(vocab)
  print(onehot_vectors)
#+END_SRC

Which produces:

#+begin_example
[[0 0 0 1 0 0 0 0 0 0]
 [0 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 1 0 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 1]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0]
 [1 0 0 0 0 0 0 0 0 0]]
#+end_example


A better visualisation of the information can be done either with
=Pandas=, which lists the table headers or with =DataFrame=, which
omits the zeros.

This way, we get (notice the explicit direction to =$pwd= and the =import=
of previous file):
#+BEGIN_SRC python :tangle code/simple_tok_pretty.py :exports code :results output
  import sys
  sys.path.append('~/repos/0-mine/compling/code/')

  import simple_tok as st
  import pandas as pd

  print("Pandas print:")
  print(pd.DataFrame(st.onehot_vectors, columns=st.vocab))

  df = pd.DataFrame(st.onehot_vectors, columns=st.vocab)
  df[df == 0] = ''                # ignore zeros
  print("Prettier, no zeros:")
  print(df)
#+END_SRC

Which produces:

#+begin_example
Pandas print:
   26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the
0    0          0           0       1    0   0      0         0   0    0
1    0          1           0       0    0   0      0         0   0    0
2    0          0           0       0    0   0      1         0   0    0
3    0          0           0       0    0   0      0         1   0    0
4    0          0           1       0    0   0      0         0   0    0
5    0          0           0       0    0   1      0         0   0    0
6    0          0           0       0    0   0      0         0   0    1
7    0          0           0       0    1   0      0         0   0    0
8    0          0           0       0    0   0      0         0   1    0
9    1          0           0       0    0   0      0         0   0    0
Prettier, no zeros:
  26. Jefferson Monticello Thomas age at began building of the
0                               1                             
1             1                                               
2                                            1                
3                                                     1       
4                        1                                    
5                                      1                      
6                                                            1
7                                   1                         
8                                                        1    
9   1        
#+end_example

* Dot Product Calculation
We will use the dot product in many situations for computing
similarity. A basic example, using =Pandas= and =numpy= is
the following:
#+BEGIN_SRC python :tangle code/dot_product.py :exports both :results output
  import pandas as pd
  import numpy as np

  v1 = pd.np.array([1, 2, 3])
  v2 = pd.np.array([2, 3, 4])
  print("The dot product of the two vectors is:")
  print(v1.dot(v2))

  print("Manual computation of the dot product:")
  print(sum([x1 * x2 for x1, x2 in zip(v1, v2)]))
#+END_SRC

which outputs:

#+RESULTS:
: The dot product of the two vectors is:
: 20
: Manual computation of the dot product:
: 20

* Better Tokenizing
Some of the most commonly used methods involve:
- =re= or =regex= (newer) packages for Regular Expression support;
- Stanford CoreNLP: accurate, less flexible, depends on Java;
- NLTK: standard;
- spaCy: newcomer.

Simple examples follow.

RegEx:
#+BEGIN_SRC python :tangle code/re_tok.py :exports both :results output
  import re

  sentence = """Thomas Jefferson began building Monticello at the\
  age of 26."""

  # regex to split at whitespace or punctuation
  # that appears at least once
  tokens = re.split(r'[-\s.,;!?]+', sentence)

  print(tokens)
#+END_SRC

which produces:

#+RESULTS:
: ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'theage', 'of', '26', '']

Precompiled RegEx passed to pattern matching:
#+BEGIN_SRC python :tangle code/pattern_tok.py :exports both :results output
  import re

  sentence = """Thomas Jefferson began building the Monticello at the\
  age of 26."""

  # prepare the regex beforehand
  pattern = re.compile(r"([-\s.,;~?])+")

  # apply the regex
  tokens = pattern.split(sentence)

  # filter out whitespace and punctuation
  good_toks = [x for x in tokens if x and x not in '- \t\n.,;~?']

  print(good_toks)
#+END_SRC

which produces:

#+RESULTS:
: ['Thomas', 'Jefferson', 'began', 'building', 'the', 'Monticello', 'at', 'theage', 'of', '26']

Using NLTK's =RegexpTokenizer=:
#+BEGIN_SRC python :tangle code/nltk_regexp_tok.py :exports both :results output
  from nltk.tokenize import RegexpTokenizer

  sentence = """Thomas Jefferson began building the Monticello at the
  age of 26."""

  tokenizer = RegexpTokenizer(r'\w+|$[0-9.]+|\S+')

  print(tokenizer.tokenize(sentence))
#+END_SRC

which gives:

#+RESULTS:
: ['Thomas', 'Jefferson', 'began', 'building', 'the', 'Monticello', 'at', 'the', 'age', 'of', '26', '.']

Using NLTK's [[http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank][=Penn Treebank Tokenizer=]]:
#+BEGIN_SRC python :tangle code/nltk_treebank_tok.py :exports both :results output
  from nltk.tokenize import TreebankWordTokenizer

  sentence = """Monticello wasn't designed as UNESCO World Heritage\
  Site until 1987."""

  tokenizer = TreebankWordTokenizer()
  print(tokenizer.tokenize(sentence))
#+END_SRC

which gives:

#+RESULTS:
: ['Monticello', 'was', "n't", 'designed', 'as', 'UNESCO', 'World', 'HeritageSite', 'until', '1987', '.']

* N-grams
In some cases, it could be relevant for the meaning of a text
to keep N-grams. For example, if the text contains "ice cream",
it is best interpreted if the two words are /not/ separated. For
such purposes, we use N-grams (2-grams in the case of "ice cream").

Example using NLTK's =ngrams= tokenizer:
#+BEGIN_SRC python :tangle code/nltk_ngrams.py :exports both :results output
  import re
  from nltk.util import ngrams

  sentence = """Thomas Jefferson began building Monticello at the
  age of 26."""

  # get the tokens (words) first with a simple split
  pattern = re.compile(r"([-\s.,;~?])+")
  tokens = pattern.split(sentence)
  # disregard whitespace and punctuation
  tokens = [x for x in tokens if x and x not in '- \t\n.,;~?']

  # N-grams with NLTK
  print("2- and 3-grams as tuples:")
  print(list(ngrams(tokens, 2)))
  print(list(ngrams(tokens, 3)))

  print("2-grams joined with whitespace:")
  two_grams = list(ngrams(tokens, 2))
  print([" ".join(x) for x in two_grams])
#+END_SRC

This gives:
#+RESULTS:
: 2- and 3-grams as tuples:
: [('Thomas', 'Jefferson'), ('Jefferson', 'began'), ('began', 'building'), ('building', 'Monticello'), ('Monticello', 'at'), ('at', 'the'), ('the', 'age'), ('age', 'of'), ('of', '26')]
: [('Thomas', 'Jefferson', 'began'), ('Jefferson', 'began', 'building'), ('began', 'building', 'Monticello'), ('building', 'Monticello', 'at'), ('Monticello', 'at', 'the'), ('at', 'the', 'age'), ('the', 'age', 'of'), ('age', 'of', '26')]
: 2-grams joined with whitespace:
: ['Thomas Jefferson', 'Jefferson began', 'began building', 'building Monticello', 'Monticello at', 'at the', 'the age', 'age of', 'of 26']

* Stopwords
NLTK contains a list of stopwords which one can readily download,
use and query:
#+BEGIN_SRC python :tangle code/nltk_stopwords.py :exports both :results output
  import nltk

  nltk.download('stopwords')
  stop_words = nltk.corpus.stopwords.words('english')
  print("There are " + str(len(stop_words)) + " stopwords in NLTK English DB")

  print("Some of them are:")
  print(stop_words[:10])

  print("The shortest are:")
  print([sw for sw in stop_words if len(sw) == 1])
#+END_SRC

Which gives:
#+RESULTS:
: There are 179 stopwords in NLTK English DB
: Some of them are:
: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]
: The shortest are:
: ['i', 'a', 's', 't', 'd', 'm', 'o', 'y']

* Normalizing the Vocabulary
*Case folding* is simply done with =.lower()=.

*Stemming* can be done manually for disregarding plural =s='s:
#+BEGIN_SRC python :tangle code/stemmer_plurals.py :exports both :results output
  import re

  def stem(phrase):
	  return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip("'")
			   for word in phrase.lower().split()])

  print("Stemming plurals for 'houses' gives: ", end="")
  print(stem('houses'))
  print("Stemming plurals from 'Doctor House's Calls' gives: ", end="")
  print(stem("Doctor House's calls"))
#+END_SRC

This gives:
#+RESULTS:
: Stemming plurals for 'houses' gives: house
: Stemming plurals from 'Doctor House's Calls' gives: doctor house call

NLTK's =PorterStemmer= works like this:
#+BEGIN_SRC python :tangle code/stemmer_nltk_porter.py :exports both :results output
  from nltk.stem.porter import PorterStemmer

  stemmer = PorterStemmer()
  print("Porter Stemmer for 'Dish washer's washed dishes' gives:")
  print(' '.join([stemmer.stem(w).strip("'") for w in
				  "Dish washer's washed dishes".split()]))
#+END_SRC

#+RESULTS:
: Porter Stemmer for 'Dish washer's washed dishes' gives:
: dish washer wash dish

The Porter Stemmer has been ported to pure Python [[https://github.com/jedijulia/porter-stemmer][here]]. The basic steps
are the following:
- *Step 1a*: remove "s" and "es" endings;
- *Step 1b*: remove "ed", "ing" and "at" endings;
- *Step 1c*: remove "y" endings;
- *Step 2*: /nounifying/ endings such as "(a)tional", "ence", "able";
- *Step 3*: adjective endings such as "icate", "ful", "alize";
- *Step 4*: adjectives and nound endings such as "ive", "ible", "ent", "ism";
- *Step 5a*: some "e" endings still left around;
- *Step 5b*: trailing double consonants for which the stem should end in one consonant.

*Lemmatization* is done readily with NLTK:
#+BEGIN_SRC python :tangle code/lemmatizer_nltk.py :exports both :results output
  import nltk
  nltk.download('wordnet')
  from nltk.stem import WordNetLemmatizer

  lemmatizer = WordNetLemmatizer()
  adjectives = ["better", "good", "goods", "best"]
  nouns = ["better", "goods", "goodness"]

  print("> We lemmatize the following adjectives:")
  for adj in adjectives:
	  print(adj, end=" ")
  print("\n> And get respectively:")
  for adj in adjectives:
	  print(lemmatizer.lemmatize(adj, pos="a"), end=" ")

  print("\n> Now the following nouns:")
  for n in nouns:
	  print(n, end=" ")
  print("\n> And get respectively:")
  for n in nouns:
	  print(lemmatizer.lemmatize(n, pos="n"), end=" ")
#+END_SRC

#+RESULTS:
: > We lemmatize the following adjectives:
: better good goods best 
: > And get respectively:
: good good goods best 
: > Now the following nouns:
: better goods goodness 
: > And get respectively:
: better good goodness 
